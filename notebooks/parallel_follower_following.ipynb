{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywren, json, requests, time\n",
    "import pandas as pd\n",
    "from json import loads\n",
    "from get_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each page, extract followers/ following\n",
    "def Follow(response):\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    follow = soup.find_all(\"td\", \"info fifty screenname\")\n",
    "    cursor = soup.find_all(\"div\", \"w-button-more\")\n",
    "    try: cursor = findall(r'cursor=(.*?)\">', str(cursor))[0]\n",
    "    except IndexError: pass  \n",
    "    follow = [i.find('a')['name'] for i in follow]\n",
    "    return follow, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## given a user, loop through its followers/ following pages using cursors\n",
    "def scrape_username(username, fol = 'followers'):\n",
    "    mobile = \"https://mobile.twitter.com\"\n",
    "    url = f\"{mobile}/{username}/{fol}?lang=en\"\n",
    "    init = '-1'\n",
    "    follower_lst = []\n",
    "    count = 0\n",
    "    \n",
    "    while(len(init)!=0):\n",
    "        if init == '-1': tar_url = url\n",
    "        else: tar_url = url + f\"&cursor={init}\"\n",
    "        print(tar_url)\n",
    "        r = requests.get(tar_url)\n",
    "        if not r: break\n",
    "        followers, init = Follow(r.text)\n",
    "        follower_lst.extend(followers)\n",
    "    return {username: follower_lst}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "## deleted paul ryan and many other user who have too much followers\n",
    "## used \"check_follower\" because I need to rescrape some users\n",
    "data = pd.read_csv('twitter_accounts.csv')\n",
    "sub_data = data[(data['check_follower']==1) & \n",
    "                (data['true_followers'] < 100000)]\n",
    "accounts = list(sub_data['twitter'])\n",
    "print(len(accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "#datasets = json.load(open('account_type.json'))\n",
    "#expert_twitters = list(set(data['twitter']))\n",
    "#tar_accounts = set([j for k in datasets for j in datasets[k]] + expert_twitters)\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "#scrape_helper = lambda username: scrape_network(username, tar_accounts)\n",
    "#futures = pwex.map(scrape_helper, accounts)\n",
    "futures = pwex.map(scrape_username, accounts)\n",
    "print(len(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r",
      "2\r",
      "3\r",
      "4\r",
      "5\r",
      "6\r",
      "7\r",
      "8\r",
      "9\r",
      "10\r",
      "11\r",
      "12\r",
      "13\r"
     ]
    }
   ],
   "source": [
    "result = pywren.get_all_results(futures)\n",
    "data = dict()\n",
    "for num, item in enumerate(result):\n",
    "    if num == 0: continue\n",
    "    print(num, end = '\\r')\n",
    "    data.update(item.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete the data in Boto3\n",
    "import boto3\n",
    "def cleanup(bucket_name):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for item in bucket.objects.all(): item.delete()      \n",
    "cleanup('chen.liang-pywren-478')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = json.load(open('followers_parallel.json'))\n",
    "for user in data: total_data.update({user: data[user]})\n",
    "json.dump(total_data, open('followers_parallel.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_data = json.load(open('following_parallel.json'))\n",
    "#for user in data: total_data.update({user: data[user]})\n",
    "#json.dump(data, open('following_parallel.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
